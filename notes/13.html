<h1 id="lecture-13">Lecture 13</h1>
<h2 id="version-control-for-open-source-packages">Version control for open source packages</h2>
<p>1.2.3 major.minor.bugFix</p>
<ul>
<li>major</li>
<li>version 0 code may or may not work in version 1 - developers reserve the right to break older version of the code<br /></li>
<li><p>there could have be a major API change</p></li>
<li><p>minor - new features added along the way</p></li>
</ul>
<h2 id="punchline-for-svm">Punchline for SVM</h2>
<ul>
<li>Kernels / distances <span class="math"><em>l</em><em>a</em><em>t</em><em>e</em><em>x</em><em>x</em><sub><em>i</em></sub>, <em>x</em><sub><em>j</em></sub></span></li>
</ul>
<h2 id="maximal-marginal-classifier">Maximal Marginal Classifier</h2>
<h2 id="classifier-techniques">Classifier techniques</h2>
<ul>
<li>random forest - classification trees or CART - decision trees<br /></li>
<li>linear discriminant analysis (p.106 elements of statistical learning)
<ul>
<li>assume multinormals and compute likelihood ratios of two populations<br /></li>
<li>assume sigma1 = sigma2 &lt;- where it is linear<br /></li>
</ul></li>
<li>quadratic discriminant analysis
<ul>
<li>assume sigma1 != sigma2</li>
<li>terms in the likelihood ratio do not cancel</li>
</ul></li>
</ul>
<h2 id="glm">GLM</h2>
<ul>
<li>Logistic regression</li>
<li><p>poisson regression</p></li>
<li>K nearest neighbors<br /></li>
<li>naive Bayes</li>
<li><span class="math"><em>P</em>(<em>y</em>∣<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . ) = <em>P</em>(<em>x</em><sub><em>i</em></sub>∣<em>x</em><sub><em>i</em> + 1</sub>, <em>y</em>. . . )</span> assume independence between parameters<br /></li>
<li>neural networks</li>
<li><p>non-naive Bayes (has to know the joint distribution)</p></li>
</ul>
<h2 id="mcmc">MCMC</h2>
<ul>
<li>sampling from posterior / complicated distributions</li>
<li>other similar techniques - importance / acception-rejection sampling, inverse CDF</li>
</ul>
<p>most are similar with different loss functions</p>
<h2 id="how-to-cut-between-two-populations">how to cut between two populations</h2>
<p>p-1 hyperplane p-2 line $ 0 = <em>{i=1}^N </em>i x_i + _0 $ or $ 0 = a + b~x$</p>
<h2 id="support-vector-something-machine-classifier">Support Vector Something (Machine, classifier)</h2>
<ul>
<li>loss function of SVM is very similar to logistic regression</li>
<li>margins between several choices of cut-off lines or what 's called support<br /></li>
<li>convex optimization - no local minima / maxima</li>
<li>optimize the <span class="math"><em>β</em><sub><em>i</em></sub></span> for the margin (distances between the two populations) to be as wide as possible<br /></li>
<li>distances of point p to the hyperplane defined by <span class="math"><em>b</em><em>e</em><em>t</em><em>a</em></span> <span class="math"><em>s⃗</em> = <em>p</em> * <em>β</em> / ∣∣<em>β</em>∣∣</span></li>
<li>constraint 1 : but we want to get rid of the magnitude of <span class="math">∣∣<em>β</em>∣∣</span> and maximize that</li>
<li>constraint 2: <span class="math">∑ <em>β</em><sub><em>i</em></sub><sup>2</sup> = 1</span></li>
</ul>
<p>AKA Maximum marginal problem - also a convex optimization problem Optimize M such that <span class="math"><em>y</em><sub><em>i</em></sub>(∑ <sub><em>i</em> = 1</sub><em>β</em><sub><em>i</em></sub><em>x</em><sub><em>i</em></sub> + <em>x</em><sub>0</sub>) ≥ <em>M</em></span></p>
<p><br /><span class="math"><em>f</em>(<em>x</em>) = (∑ <sub><em>i</em> = 1</sub><em>β</em><sub><em>i</em></sub><em>x</em><sub><em>i</em></sub> + <em>x</em><sub>0</sub>) &gt; 0<em>y</em> = 1</span><br /> <br /><span class="math"><em>f</em>(<em>x</em>) = (∑ <sub><em>i</em> = 1</sub><em>β</em><sub><em>i</em></sub><em>x</em><sub><em>i</em></sub> + <em>x</em><sub>0</sub>) &lt; 0<em>y</em> =  − 1</span><br /></p>
<p><span class="math">$y_i f(x_i) \geq M(1-\episilon_i)$</span> where <span class="math"><em>ε</em><sub><em>i</em></sub> &gt; 0</span> i = 1,...,n $ _i^n &lt; constant$ constant is a tuning parameter<br />if want clean separation make constant = 0 or if we are more tolerant , make constant large * every point within the margin are supports</p>
<p>two steps to find tuning parameter * optimize it out - may overfit * do cross validation * bias - variance trade-off</p>
<p>if <span class="math"><em>e</em><em>p</em><em>s</em><em>i</em><em>l</em><em>o</em><em>n</em> &gt; 1 − <em>t</em><em>h</em><em>e</em><em>p</em><em>o</em><em>i</em><em>n</em><em>t</em><em>s</em><em>w</em><em>i</em><em>l</em><em>l</em><em>b</em><em>e</em><em>o</em><em>n</em><em>t</em><em>h</em><em>e</em><em>w</em><em>r</em><em>o</em><em>n</em><em>g</em><em>s</em><em>i</em><em>d</em><em>e</em></span></p>
<h2 id="redo-problem">Redo problem</h2>
<p>this time - maximize margins and allow a few points to be on the wrong side of the line</p>
<h2 id="knn">KNN</h2>
<ul>
<li>needs to keep all data around - SVM doesn't have to</li>
</ul>
<h2 id="svm">SVM</h2>
<p>Can rewrite <span class="math"><em>f</em>(<em>x</em><sub><em>i</em></sub>) = <em>β</em> + ∑ <sub><em>i</em></sub><sup><em>N</em></sup><em>α</em><sub><em>i</em></sub> &lt; <em>x</em><sub><em>i</em></sub><em>x</em><sub><em>j</em></sub> &gt; </span> where <span class="math"> &lt;  &gt; </span> represents the inner product * <span class="math"><em>α</em><sub><em>i</em></sub> = 0</span> for a lot of i we need to know where <span class="math"><em>α</em><sub><em>i</em></sub> ≠ 0</span> for the actual support vectors! can get the support vectors when we do optimization.</p>
<ul>
<li>can replace the inner product metric with more complicated ones e.g. <span class="math"><em>K</em>(<em>x</em><sub><em>i</em></sub>, <em>x</em><sub><em>j</em></sub>) = (1 + ∑ <em>x</em><sub><em>i</em></sub><em>x</em><sub><em>j</em></sub>)<sup><em>d</em></sup></span></li>
<li>multi-dimensional scaling - dimension reduction</li>
</ul>
<p>Radial basis - <span class="math"><em>e</em><em>x</em><em>p</em>( − <em>d</em>(∣<em>x</em> − <em>x</em><sub><em>i</em></sub>∣<sup>2</sup>)</span></p>
<h2 id="classify-k-classes">classify K classes</h2>
<p>assume ABCD... classes</p>
<p>approach 1) only do ABD, C then repeat SVM for ABD greedy algorithm</p>
<p>one-versus-all-the-others.....</p>
<p>approach 2) A, BCD or B, ACD or C, BAD etc. then take a vote not greedy<br />pick largest <span class="math"><em>l</em><em>a</em><em>t</em><em>e</em><em>x</em><em>f</em><sub><em>β</em></sub>(<em>x</em>)</span></p>
<p>ensemble approach</p>
<p>can weight the votes</p>
<h2 id="recommended-reading">Recommended reading</h2>
<ul>
<li>Elements of statistical learning</li>
<li>pattern recognition and machine learning - Christopher M. Bishop</li>
<li>Kernel Methods for Pattern Analysis - J. Shawe-Taylor and N. Cristiani<br /></li>
<li>Elements of statistical learning for dummies<br /></li>
<li>Kevin Murphy Machine Learning: A Probabilistic Perspective- machine learning book discusses about different kernels</li>
</ul>
