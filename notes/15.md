Lecture 15
==========
* boosting - can have many features / choices

* classification tree is harder to implement than regression trees 
	* needs to have a measure of impurity  
	  	* Gini coefficient 
		* Entropy 
	* Random forest 
		* relatively easy 
		* implement a stump  
		* also only use a subset of the features 
* Boosting 
  	* rpart(formula, data, weights)
	* $$\sum_i w_i = 1/n$$


StackOverFlow hw
---------------
* use NLP to process the questions to process them into variables 
  	* ignore case sensitivity 
	* ignore tense sensitivity - stemming  
	* some part of speech (POS) should be more important than the others 
		* seems to be hard to understand the context 
		* proper noun vs noun?!?!
		* [natural language processing library tagset](http://www.computing.dcu.ie/~acahill/tagset.html) 
	* how to treat punctuation  
	* remove stop words - a / the / is after detecting the rest of the
 part of speech
	* get subsets of word so "not something" is one pair 
	* get bag of words from the question ...
* may not be good idea to do sampling - may sample only 2% from 6% of closed sample
* look at the 6% 
* user reputation cannot go below 1
* normalize the post or find
	* percent of # of stop words 
	* amount of code 
* look at conditional probability 
  	* word | closed - naive Bayes approach  
$$P(closed | w_1, w_2, ..., w_n) \propto P(w_1, w_2,..., w_n |closed)
P(closed)$$
$$= P(w_1 | closed) P(w_2 | closed)... P(w_3 | closed) ...$$
assuming independence
or
$$P(open | w_1, w_2, ..., w_n) \propto P(w_1, w_2,..., w_n |open) P(open)$$
The evidence for both formulae should be the same for comparison purposes????

measure the distance of different questions - could use KNN 
---------------------------------
* use term frequency (TF) to 
* use inverse document frequency (IDF)
* tags on the questions - and see if they are relevant  
* also can pull upvote statistics - full dump 36G StackOverFlow in xml 

XML
---
* entities - weird words 
```r
doc = xmlParse("users.xml")
xmlToDataFrame()
```
* getNodeSet() - @ is the attribute  
* .Last.value <- R stores the last variable 
* do it in parallel?

Careful not to overfit!

R package 
--------
* rpart
* if slow, use 
	* parallelR
	* Mahout 
	* or rewrite part of XML in C 

* 288 s to get 12 GB of XML files on Hilbert 
* write C code to sweep over all the nodes separated by <tags></tags> or
attributes  
* sometimes cannot read one line at a time - have to cut out each node  
* use Simple API for XML - use SAX in order cut out each node  
* event driven programming - asynchronous programming  

differences between ML and statistics 
* no model for ML
* not fitting a model 
* not asking why 

Possible solution to dealing with [orange tree problem] 
(https://docs.google.com/presentation/d/1Xad89ZfGzlSk_JFERzp9RQQUt26YACoao97ir15epb0/edit#slide=id.g1c73e4298_026)
logistical regression - classification
multinomial logistic regression with 3 response variables 

for random forest:
use out of bag samples for doing cross validation !

depends on how the number of parameters is for a certain 
don't know the parametrization 

why not just average the outputs of all 3 methods?!
* see what each method misclassifies 
* see why each method misses the variables - tell you which bit would be
hard to predict  


data exploration - eyeball the data
---------------
* plotting and plotting to detect structure
* do projection pursue


